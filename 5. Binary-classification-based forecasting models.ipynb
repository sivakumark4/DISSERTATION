{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88a8c8",
   "metadata": {},
   "source": [
    "# 5. Binary-classification-based forecasting models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46287e6",
   "metadata": {},
   "source": [
    "### 5.1. Testing all Binary-classification-based forecasting models for the month of APRIL 2021 to select the best two models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b1b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.8159722222222222\n",
      "Sensitivity: 0.4845605700712589\n",
      "Confusion Matrix: \n",
      "[[971  48]\n",
      " [217 204]]\n",
      "\n",
      "Model: k-Nearest Neighbors\n",
      "Accuracy: 0.7875\n",
      "Sensitivity: 0.5130641330166271\n",
      "Confusion Matrix: \n",
      "[[918 101]\n",
      " [205 216]]\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.7951388888888888\n",
      "Sensitivity: 0.39429928741092635\n",
      "Confusion Matrix: \n",
      "[[979  40]\n",
      " [255 166]]\n",
      "\n",
      "Model: Artificial Neural Network\n",
      "Accuracy: 0.8125\n",
      "Sensitivity: 0.5320665083135392\n",
      "Confusion Matrix: \n",
      "[[946  73]\n",
      " [197 224]]\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "file_path = 'Database_1_capped.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Converting 'Date and Time' to datetime \n",
    "data['Date and Time'] = pd.to_datetime(data['Date and Time'], format='%d/%m/%Y %H:%M')\n",
    "data.set_index('Date and Time', inplace=True)\n",
    "\n",
    "# Splitting data into training and test periods\n",
    "train_start = '2019-01-01'\n",
    "train_end = '2021-03-31'\n",
    "test_start = '2021-04-01'\n",
    "test_end = '2021-04-30'\n",
    "\n",
    "train_data = data[train_start:train_end]\n",
    "test_data = data[test_start:test_end]\n",
    "\n",
    "# Separating features and target variable\n",
    "X_train = train_data.drop(columns=['Demand_Capped'])\n",
    "y_train = train_data['Demand_Capped']\n",
    "X_test = test_data.drop(columns=['Demand_Capped'])\n",
    "y_test = test_data['Demand_Capped']\n",
    "\n",
    "# Converting target variable to binary (e.g., threshold at median value)\n",
    "threshold = y_train.median()\n",
    "y_train = (y_train > threshold).astype(int)\n",
    "y_test = (y_test > threshold).astype(int)\n",
    "\n",
    "# One-Hot Encode categorical features\n",
    "categorical_features = ['REGION', 'Holiday', 'Season']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "encoded_categorical = encoder.fit_transform(X_train[categorical_features])\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "X_train_numerical = X_train.drop(columns=categorical_features).values\n",
    "X_train_processed = np.concatenate([X_train_numerical, encoded_categorical], axis=1)\n",
    "\n",
    "# Applying the same transformation to test data\n",
    "encoded_categorical_test = encoder.transform(X_test[categorical_features])\n",
    "X_test_numerical = X_test.drop(columns=categorical_features).values\n",
    "X_test_processed = np.concatenate([X_test_numerical, encoded_categorical_test], axis=1)\n",
    "\n",
    "# Handling class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "# Scaling the features after SMOTE\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "# Defining models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Artificial Neural Network': MLPClassifier(hidden_layer_sizes=(20,), max_iter=2000, random_state=42)\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Training the model\n",
    "    model.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculating evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # Sensitivity (recall for PELDs)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Storing results\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Confusion Matrix': confusion\n",
    "    }\n",
    "\n",
    "# Printing the results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']}\")\n",
    "    print(f\"Sensitivity: {metrics['Sensitivity']}\")\n",
    "    print(f\"Confusion Matrix: \\n{metrics['Confusion Matrix']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28836c6b",
   "metadata": {},
   "source": [
    "#### RESULT:\n",
    "\n",
    "Individual performances of four different binary classification models:\n",
    "\n",
    "     Model: Logistic Regression\n",
    "     Accuracy: 0.8159722222222222\n",
    "     Sensitivity: 0.4845605700712589\n",
    "\n",
    "     Model: k-Nearest Neighbors\n",
    "     Accuracy: 0.7875\n",
    "     Sensitivity: 0.5130641330166271\n",
    "\n",
    "\n",
    "     Model: Random Forest\n",
    "     Accuracy: 0.7951388888888888\n",
    "     Sensitivity: 0.39429928741092635\n",
    "\n",
    "\n",
    "     Model: Artificial Neural Network\n",
    "     Accuracy: 0.8125\n",
    "     Sensitivity: 0.5320665083135392\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6c9fa",
   "metadata": {},
   "source": [
    "So we have finalised two models for our hybrid model and those are Logistic Regression and Artificial Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f74b7",
   "metadata": {},
   "source": [
    "### 5.2 Logistic Regression model(Tested for 2021 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5f7efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n",
      "Balanced Accuracy: 0.8443256090314915\n",
      "Sensitivity: 0.9411764705882353\n",
      "Confusion Matrix: \n",
      "[[222  75]\n",
      " [  4  64]]\n",
      "\n",
      "Logistic Regression (LR) PLED Counts:\n",
      "PLED_Prediction\n",
      "0    226\n",
      "1    139\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Actual PLED Count: 68\n",
      "Forecasted PLED Count: 139\n",
      "Matching PLED Count: 64\n",
      "Matching Non-PLED Count: 222\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "file_path = 'Database_1_capped.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Converting 'Date and Time' to datetime \n",
    "data['Date and Time'] = pd.to_datetime(data['Date and Time'], format='%d/%m/%Y %H:%M')\n",
    "data.set_index('Date and Time', inplace=True)\n",
    "\n",
    "# Defining the fixed training and test periods\n",
    "train_start = '2019-01-01'\n",
    "train_end = '2020-12-31'\n",
    "test_start = '2021-01-01'\n",
    "test_end = '2021-12-31'\n",
    "\n",
    "train_data = data[train_start:train_end]\n",
    "test_data = data[test_start:test_end]\n",
    "\n",
    "# Separating features and target variable\n",
    "X_train = train_data.drop(columns=['Demand_Capped'])\n",
    "y_train = train_data['Demand_Capped']\n",
    "X_test = test_data.drop(columns=['Demand_Capped'])\n",
    "y_test = test_data['Demand_Capped']\n",
    "\n",
    "# Converting target variable to binary using a threshold based on the 95th percentile for training data\n",
    "threshold = train_data['Demand_Capped'].quantile(0.95)\n",
    "y_train = (y_train > threshold).astype(int)\n",
    "\n",
    "# Converting target variable to binary for test data using actual peak load days\n",
    "test_threshold = test_data['Demand_Capped'].quantile(0.95)\n",
    "y_test = (y_test > test_threshold).astype(int)\n",
    "\n",
    "# One-Hot Encode categorical features (including temporal features)\n",
    "categorical_features = ['REGION', 'Holiday', 'Season', 'Month', 'Day', 'Hour']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "encoded_categorical = encoder.fit_transform(X_train[categorical_features])\n",
    "\n",
    "# Concatenate encoded features with numerical features, including weather data\n",
    "numerical_features = X_train.drop(columns=categorical_features).values\n",
    "X_train_processed = np.concatenate([numerical_features, encoded_categorical], axis=1)\n",
    "\n",
    "# Applying the same transformation to test data\n",
    "encoded_categorical_test = encoder.transform(X_test[categorical_features])\n",
    "X_test_numerical = X_test.drop(columns=categorical_features).values\n",
    "X_test_processed = np.concatenate([X_test_numerical, encoded_categorical_test], axis=1)\n",
    "\n",
    "# Handling class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "# Scaling the features after SMOTE\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "# Training the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, penalty='l2', class_weight='balanced')\n",
    "lr_model.fit(X_train_scaled, y_train_balanced)\n",
    "\n",
    "# Predicting on the test set using Logistic Regression\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Creating a DataFrame to save 30-minute predictions for Logistic Regression\n",
    "lr_prediction_df = test_data.copy()\n",
    "lr_prediction_df['PLED_Prediction'] = y_pred_lr\n",
    "\n",
    "# Aggregating predictions to daily level for Logistic Regression\n",
    "lr_prediction_df['Date'] = lr_prediction_df.index.date\n",
    "daily_lr_prediction = lr_prediction_df.groupby('Date')['PLED_Prediction'].max().reset_index()\n",
    "daily_lr_prediction['PLED_Prediction'] = daily_lr_prediction['PLED_Prediction'].astype(int)\n",
    "\n",
    "# Including actual values in the output CSV\n",
    "daily_lr_prediction['Actual_PLED'] = y_test.groupby(y_test.index.date).max().values\n",
    "\n",
    "# Saving the daily results to CSV for Logistic Regression\n",
    "lr_output_file = \"Logistic_Regression_PLED_Classification_Daily999.csv\"\n",
    "daily_lr_prediction.to_csv(lr_output_file, index=False)\n",
    "\n",
    "# Calculating evaluation metrics for Logistic Regression\n",
    "daily_y_test = y_test.groupby(y_test.index.date).max()  # Aggregate ground truth to daily level\n",
    "balanced_accuracy_lr = balanced_accuracy_score(daily_y_test, daily_lr_prediction['PLED_Prediction'])\n",
    "sensitivity_lr = recall_score(daily_y_test, daily_lr_prediction['PLED_Prediction'], pos_label=1)  # Sensitivity (recall for PELDs)\n",
    "confusion_lr = confusion_matrix(daily_y_test, daily_lr_prediction['PLED_Prediction'])\n",
    "\n",
    "# Printing results for Logistic Regression\n",
    "print(f\"\\nModel: Logistic Regression\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_lr}\")\n",
    "print(f\"Sensitivity: {sensitivity_lr}\")\n",
    "print(f\"Confusion Matrix: \\n{confusion_lr}\")\n",
    "\n",
    "# Counting number of PLEDs (1) and Non-PLEDs (0) \n",
    "def count_pled_nonpled(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    return df['PLED_Prediction'].value_counts()\n",
    "\n",
    "# File path for the output CSV\n",
    "lr_csv_path = 'Logistic_Regression_PLED_Classification_Daily999.csv'\n",
    "\n",
    "# Counting PLEDs and Non-PLEDs for Logistic Regression\n",
    "lr_counts = count_pled_nonpled(lr_csv_path)\n",
    "\n",
    "print(\"\\nLogistic Regression (LR) PLED Counts:\")\n",
    "print(lr_counts)\n",
    "\n",
    "# Comparing Actual vs Forecasted PLEDs and Non-PLEDs\n",
    "actual_vs_forecast_df = pd.DataFrame({\n",
    "    'Date': daily_y_test.index,\n",
    "    'Actual': daily_y_test.values,\n",
    "    'Forecast': daily_lr_prediction['PLED_Prediction']\n",
    "})\n",
    "\n",
    "actual_pled_count = actual_vs_forecast_df[actual_vs_forecast_df['Actual'] == 1].shape[0]\n",
    "forecast_pled_count = actual_vs_forecast_df[actual_vs_forecast_df['Forecast'] == 1].shape[0]\n",
    "matching_pled_count = actual_vs_forecast_df[(actual_vs_forecast_df['Actual'] == 1) & (actual_vs_forecast_df['Forecast'] == 1)].shape[0]\n",
    "non_pled_count = actual_vs_forecast_df[(actual_vs_forecast_df['Actual'] == 0) & (actual_vs_forecast_df['Forecast'] == 0)].shape[0]\n",
    "\n",
    "print(f\"\\nActual PLED Count: {actual_pled_count}\")\n",
    "print(f\"Forecasted PLED Count: {forecast_pled_count}\")\n",
    "print(f\"Matching PLED Count: {matching_pled_count}\")\n",
    "print(f\"Matching Non-PLED Count: {non_pled_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c56eea",
   "metadata": {},
   "source": [
    "#### Result : \n",
    "     Model: Logistic Regression\n",
    "            Balanced Accuracy: 0.8443256090314915\n",
    "            Sensitivity: 0.9411764705882353"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f659a7e",
   "metadata": {},
   "source": [
    "we will use this csv \"Logistic_Regression_PLED_Classification_Daily999.csv\" to feed the meta classifier hybrid model. this csv is having pelds forecasted by LOGISTIC REGRESSION MODEL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4c6da",
   "metadata": {},
   "source": [
    "### 5.3 Artificial Neural Network (ANN) model (Tested for 2021 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2aa096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/v941jp4540374j7s4hp2b37w0000gn/T/ipykernel_22839/2308178613.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['Demand_12AM'] = train_data.groupby(train_data.index.date)['Demand_Capped'].transform('first')\n",
      "/var/folders/tn/v941jp4540374j7s4hp2b37w0000gn/T/ipykernel_22839/2308178613.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['Demand_12AM'] = test_data.groupby(test_data.index.date)['Demand_Capped'].transform('first')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual Peak Days: Index([2021-06-14, 2021-06-15, 2021-06-16, 2021-06-17, 2021-06-18, 2021-06-21,\n",
      "       2021-06-27, 2021-06-28, 2021-06-30, 2021-07-01, 2021-07-02, 2021-07-06,\n",
      "       2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12,\n",
      "       2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18,\n",
      "       2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-27,\n",
      "       2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02,\n",
      "       2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-09, 2021-08-10,\n",
      "       2021-08-11, 2021-08-12, 2021-08-13, 2021-08-14, 2021-08-15, 2021-08-16,\n",
      "       2021-08-17, 2021-08-25, 2021-08-26, 2021-08-27, 2021-08-28, 2021-08-29,\n",
      "       2021-08-30, 2021-09-04, 2021-09-05, 2021-09-06, 2021-09-07, 2021-09-08,\n",
      "       2021-09-09, 2021-09-10, 2021-09-11, 2021-09-12, 2021-09-13, 2021-09-14,\n",
      "       2021-09-21, 2021-09-22],\n",
      "      dtype='object')\n",
      "Actual PLED Count: 68\n",
      "\n",
      "Model: Artificial Neural Network\n",
      "Balanced Accuracy: 0.863834422657952\n",
      "Sensitivity: 0.7647058823529411\n",
      "Confusion Matrix: \n",
      "[[286  11]\n",
      " [ 16  52]]\n",
      "\n",
      "Artificial Neural Network (ANN) PLED Counts:\n",
      "PLED_Prediction\n",
      "0    302\n",
      "1     63\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Forecasted PLED Count: 63\n",
      "Matching PLED Count: 52\n",
      "Matching Non-PLED Count: 286\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "file_path = 'Database_1_capped.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Converting 'Date and Time' to datetime \n",
    "data['Date and Time'] = pd.to_datetime(data['Date and Time'], format='%d/%m/%Y %H:%M')\n",
    "data.set_index('Date and Time', inplace=True)\n",
    "\n",
    "# Defining the fixed training and test periods\n",
    "train_start = '2019-01-01'\n",
    "train_end = '2020-12-31'\n",
    "test_start = '2021-01-01'\n",
    "test_end = '2021-12-31'\n",
    "\n",
    "train_data = data[train_start:train_end]\n",
    "test_data = data[test_start:test_end]\n",
    "\n",
    "# Addding '12 AM' demand value as a feature\n",
    "train_data['Demand_12AM'] = train_data.groupby(train_data.index.date)['Demand_Capped'].transform('first')\n",
    "test_data['Demand_12AM'] = test_data.groupby(test_data.index.date)['Demand_Capped'].transform('first')\n",
    "\n",
    "# Separatng features and target variable\n",
    "X_train = train_data.drop(columns=['Demand_Capped'])\n",
    "y_train = train_data['Demand_Capped']\n",
    "X_test = test_data.drop(columns=['Demand_Capped'])\n",
    "y_test = test_data['Demand_Capped']\n",
    "\n",
    "# Converting target variable to binary using a threshold based on the 95th percentile for training data\n",
    "y_train_threshold = train_data['Demand_Capped'].quantile(0.95)\n",
    "y_train = (y_train > y_train_threshold).astype(int)\n",
    "\n",
    "# Converting target variable to binary for test data using actual peak load days\n",
    "test_threshold = test_data['Demand_Capped'].quantile(0.95)\n",
    "y_test = (y_test > test_threshold).astype(int)\n",
    "\n",
    "# One-Hot Encode categorical features (including temporal features)\n",
    "categorical_features = ['REGION', 'Holiday', 'Season', 'Month', 'Day', 'Hour']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "encoded_categorical = encoder.fit_transform(X_train[categorical_features])\n",
    "\n",
    "# Concatenate encoded features with numerical features, including weather data\n",
    "numerical_features = X_train.drop(columns=categorical_features).values\n",
    "X_train_processed = np.concatenate([numerical_features, encoded_categorical], axis=1)\n",
    "\n",
    "# Applying the same transformation to test data\n",
    "encoded_categorical_test = encoder.transform(X_test[categorical_features])\n",
    "X_test_numerical = X_test.drop(columns=categorical_features).values\n",
    "X_test_processed = np.concatenate([X_test_numerical, encoded_categorical_test], axis=1)\n",
    "\n",
    "# Handling class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "# Scaling the features after SMOTE\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "# Training the Artificial Neural Network (ANN) model using advanced training\n",
    "ann_model = MLPClassifier(hidden_layer_sizes=(150, 100), activation='relu', solver='adam', max_iter=3000, random_state=42, learning_rate='adaptive')\n",
    "ann_model.fit(X_train_scaled, y_train_balanced)\n",
    "\n",
    "# Predicting on the test set using ANN\n",
    "y_pred_ann = ann_model.predict(X_test_scaled)\n",
    "\n",
    "# Creating a DataFrame to save 30-minute predictions for ANN\n",
    "ann_prediction_df = test_data.copy()\n",
    "ann_prediction_df['PLED_Prediction'] = y_pred_ann\n",
    "\n",
    "# Aggregating predictions to daily level for ANN\n",
    "ann_prediction_df['Date'] = ann_prediction_df.index.date\n",
    "daily_ann_prediction = ann_prediction_df.groupby('Date')['PLED_Prediction'].max().reset_index()\n",
    "daily_ann_prediction['PLED_Prediction'] = daily_ann_prediction['PLED_Prediction'].astype(int)\n",
    "\n",
    "# Including actual values in the output CSV\n",
    "daily_ann_prediction['Actual_PLED'] = y_test.groupby(y_test.index.date).max().values\n",
    "\n",
    "# Saving the daily results to CSV for ANN\n",
    "ann_output_file = \"Artificial_Neural_Network_PLED_Classification_Daily999.csv\"\n",
    "daily_ann_prediction.to_csv(ann_output_file, index=False)\n",
    "\n",
    "# Calculating evaluation metrics for ANN\n",
    "daily_y_test = y_test.groupby(y_test.index.date).max()  # Aggregate ground truth to daily level\n",
    "balanced_accuracy_ann = balanced_accuracy_score(daily_y_test, daily_ann_prediction['PLED_Prediction'])\n",
    "sensitivity_ann = recall_score(daily_y_test, daily_ann_prediction['PLED_Prediction'], pos_label=1)  # Sensitivity (recall for PELDs)\n",
    "confusion_ann = confusion_matrix(daily_y_test, daily_ann_prediction['PLED_Prediction'])\n",
    "\n",
    "# Calculating actual peak days with the test set\n",
    "actual_peak_days = daily_y_test[daily_y_test == 1].index\n",
    "actual_pled_count = len(actual_peak_days)\n",
    "print(f\"\\nActual Peak Days: {actual_peak_days}\")\n",
    "print(f\"Actual PLED Count: {actual_pled_count}\")\n",
    "\n",
    "# Printing results for ANN\n",
    "print(f\"\\nModel: Artificial Neural Network\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_ann}\")\n",
    "print(f\"Sensitivity: {sensitivity_ann}\")\n",
    "print(f\"Confusion Matrix: \\n{confusion_ann}\")\n",
    "\n",
    "# Counting number of PLEDs (1) and Non-PLEDs (0) \n",
    "def count_pled_nonpled(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    return df['PLED_Prediction'].value_counts()\n",
    "\n",
    "# File path for the output CSV\n",
    "ann_csv_path = 'Artificial_Neural_Network_PLED_Classification_Daily999.csv'\n",
    "\n",
    "# Counting PLEDs and Non-PLEDs for ANN\n",
    "ann_counts = count_pled_nonpled(ann_csv_path)\n",
    "\n",
    "print(\"\\nArtificial Neural Network (ANN) PLED Counts:\")\n",
    "print(ann_counts)\n",
    "\n",
    "# Comparing Actual vs Forecasted PLEDs and Non-PLEDs\n",
    "actual_vs_forecast_df = pd.DataFrame({\n",
    "    'Date': daily_y_test.index,\n",
    "    'Actual': daily_y_test.values,\n",
    "    'Forecast': daily_ann_prediction['PLED_Prediction']\n",
    "})\n",
    "\n",
    "forecast_pled_count = actual_vs_forecast_df[actual_vs_forecast_df['Forecast'] == 1].shape[0]\n",
    "matching_pled_count = actual_vs_forecast_df[(actual_vs_forecast_df['Actual'] == 1) & (actual_vs_forecast_df['Forecast'] == 1)].shape[0]\n",
    "non_pled_count = actual_vs_forecast_df[(actual_vs_forecast_df['Actual'] == 0) & (actual_vs_forecast_df['Forecast'] == 0)].shape[0]\n",
    "\n",
    "print(f\"\\nForecasted PLED Count: {forecast_pled_count}\")\n",
    "print(f\"Matching PLED Count: {matching_pled_count}\")\n",
    "print(f\"Matching Non-PLED Count: {non_pled_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768563dc",
   "metadata": {},
   "source": [
    "#### RESULT: \n",
    "\n",
    "     Model: Artificial Neural Network\n",
    "                Balanced Accuracy: 0.863834422657952\n",
    "                Sensitivity: 0.7647058823529411\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9088ae",
   "metadata": {},
   "source": [
    "we will use this csv \"Artificial_Neural_Network_PLED_Classification_Daily999.csv\" to feed the meta classifier hybrid model. this csv is having pelds forecasted by Artificial Neural Network MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca150e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
